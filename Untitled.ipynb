{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a072c2-6c5e-4d28-870f-99f601a4d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (1.13.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from scikit-learn) (3.4.0)\n",
      "Requirement already satisfied: torch in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.3.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torchvision) (2.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from torch==2.3.0->torchvision) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /users/hutruon/miniconda3/envs/env/lib/python3.9/site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86076b8-83eb-44a6-95bc-5431d35bd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71a107a-3413-44d4-8996-7cbb62bddf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f98c2d2d-f2e6-489e-8149-0bf1367bf765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed.\n"
     ]
    }
   ],
   "source": [
    "# Define the directories containing the positive and negative files\n",
    "negative_dir = '/users/hutruon/Assignment-1-CS490/Data/NegativeFile'\n",
    "positive_dir = '/users/hutruon/Assignment-1-CS490/Data/PositiveFile'\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = '/users/hutruon/Assignment-1-CS490/Data/merged_data.txt'\n",
    "\n",
    "# List of negative and positive files\n",
    "negative_files = os.listdir(negative_dir)\n",
    "positive_files = os.listdir(positive_dir)\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Read the contents of a file and return them as a list.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.readlines()\n",
    "\n",
    "def merge_files(pos_file_path, neg_file_path, output_file, label_pos='1', label_neg='0'):\n",
    "    \"\"\"Append lines from positive and negative files to the output file with labels.\"\"\"\n",
    "    pos_lines = read_file(pos_file_path)\n",
    "    neg_lines = read_file(neg_file_path)\n",
    "    \n",
    "    with open(output_file, 'a') as file:\n",
    "        for line in pos_lines:\n",
    "            file.write(f\"{label_pos} {line}\")\n",
    "        for line in neg_lines:\n",
    "            file.write(f\"{label_neg} {line}\")\n",
    "\n",
    "# Ensure the output file is empty or does not exist before starting\n",
    "if os.path.exists(output_file_path):\n",
    "    os.remove(output_file_path)\n",
    "\n",
    "# Merge files with matching identifiers\n",
    "for pos_file in positive_files:\n",
    "    # Construct the corresponding negative file name\n",
    "    neg_file = pos_file.replace(\"_100nt.txt\", \"_negative_100nt.txt\")\n",
    "    if neg_file in negative_files:\n",
    "        merge_files(os.path.join(positive_dir, pos_file), os.path.join(negative_dir, neg_file), output_file_path)\n",
    "    else:\n",
    "        print(f\"Matching negative file not found for {pos_file}\")\n",
    "\n",
    "print(\"Merging completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3701a717-3281-4f34-a83c-aafcf0518f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training (1300592 lines), testing (346824 lines), and validation (86707 lines).\n"
     ]
    }
   ],
   "source": [
    "# Path to the merged data file\n",
    "merged_file_path = '/users/hutruon/Assignment-1-CS490/Data/merged_data.txt'\n",
    "\n",
    "# Paths for the output files\n",
    "train_file_path = '/users/hutruon/Assignment-1-CS490/Data/train_data.txt'\n",
    "test_file_path = '/users/hutruon/Assignment-1-CS490/Data/test_data.txt'\n",
    "validation_file_path = '/users/hutruon/Assignment-1-CS490/Data/validation_data.txt'\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Read the contents of a file and return them as a list.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "def write_file(file_path, lines):\n",
    "    \"\"\"Write the given lines to a file.\"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "# Read the merged data\n",
    "data_lines = read_file(merged_file_path)\n",
    "\n",
    "# Shuffle the data to ensure randomness\n",
    "# It's important for machine learning models to be trained on data that's in a random order.\n",
    "import random\n",
    "random.shuffle(data_lines)\n",
    "\n",
    "# Split data into training (75%), and a temporary set (25%)\n",
    "train_lines, temp_lines = train_test_split(data_lines, test_size=0.25, random_state=42)\n",
    "\n",
    "# Split the temporary set into testing (20% of total) and validation (5% of total)\n",
    "# Since the temporary set is 25% of the total, we'll allocate 80% of it to testing and 20% to validation\n",
    "# which corresponds to 20% and 5% of the total data, respectively.\n",
    "test_lines, validation_lines = train_test_split(temp_lines, test_size=0.2, random_state=42)  # 0.2 * 0.25 = 0.05\n",
    "\n",
    "# Write the split data to their respective files\n",
    "write_file(train_file_path, train_lines)\n",
    "write_file(test_file_path, test_lines)\n",
    "write_file(validation_file_path, validation_lines)\n",
    "\n",
    "print(f\"Data split into training ({len(train_lines)} lines), testing ({len(test_lines)} lines), and validation ({len(validation_lines)} lines).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73f7007d-2af4-4ec0-b20a-9954f7977457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(sequence):\n",
    "    \"\"\"Converts a DNA sequence to a one-hot encoded numpy array.\"\"\"\n",
    "    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
    "    one_hot_encoded = [mapping.get(nucleotide, [0, 0, 0, 0]) for nucleotide in sequence]\n",
    "    return np.array(one_hot_encoded)\n",
    "\n",
    "def read_and_encode(file_path):\n",
    "    labels = []\n",
    "    encoded_sequences = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, sequence = line.strip().split(maxsplit=1)\n",
    "            labels.append(int(label))\n",
    "            encoded_sequences.append(one_hot_encoder(sequence))\n",
    "    return np.array(labels), np.array(encoded_sequences)\n",
    "\n",
    "# Set the path to your test_data.txt file\n",
    "file_path = '/users/hutruon/Assignment-1-CS490/Data/test_data.txt'\n",
    "\n",
    "# Read and encode the sequences\n",
    "labels, encoded_sequences = read_and_encode(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9231a6a7-a8a2-4c1b-9d10-67b25c01b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         ...,\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         ...,\n",
      "         [0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 1., 0., 0.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1.],\n",
      "         [1., 0., 0., 0.],\n",
      "         [0., 0., 0., 1.],\n",
      "         ...,\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]]) tensor([0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
      "        0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0,\n",
      "        1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
      "        0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Step 3: One-hot encoding function\n",
    "def one_hot_encoder(sequence):\n",
    "    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
    "    return np.array([mapping.get(nucleotide, [0, 0, 0, 0]) for nucleotide in sequence])\n",
    "\n",
    "# Function to read and encode data from a file\n",
    "def read_data_and_encode(file_path):\n",
    "    labels = []\n",
    "    encoded_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, sequence = line.strip().split(' ', 1)\n",
    "            labels.append(int(label))\n",
    "            encoded_data.append(one_hot_encoder(sequence))\n",
    "    return np.array(labels), np.array(encoded_data)\n",
    "\n",
    "# Custom Dataset class\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.data = torch.stack([torch.tensor(s, dtype=torch.float32) for s in sequences])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 4: Read and encode data\n",
    "    labels, encoded_data = read_data_and_encode('test_data.txt')\n",
    "    \n",
    "    # Step 8: Instantiate the dataset\n",
    "    test_dataset = DNADataset(encoded_data, labels)\n",
    "    \n",
    "    # Step 9: Create a DataLoader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=True)\n",
    "    \n",
    "    # Step 10: Iterate and print one batch\n",
    "    for data, label in test_dataloader:\n",
    "        print(data, label)\n",
    "        break  # Only print the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fda294c0-bbf4-4445-84b5-0536d775920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv1d(4, 96, 11, stride=4, padding=5)  # Add padding\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Layer 2\n",
    "        self.conv2 = nn.Conv1d(96, 96, 1, padding=1)  # Add padding\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Layer 3\n",
    "        self.conv3 = nn.Conv1d(96, 96, 1, padding=1)  # Add padding\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(3, stride=2)\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "\n",
    "        # Layer 4\n",
    "        self.conv4 = nn.Conv1d(96, 192, 11, stride=4, padding=5)  # Add padding\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # Layer 5\n",
    "        self.conv5 = nn.Conv1d(192, 192, 1, padding=1)  # Add padding\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # Layer 6\n",
    "        self.conv6 = nn.Conv1d(192, 192, 1, padding=1)  # Add padding\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.pool6 = nn.MaxPool1d(3, stride=2)\n",
    "        self.drop6 = nn.Dropout(0.5)\n",
    "\n",
    "        # Layer 7\n",
    "        self.conv7 = nn.Conv1d(192, 384, 3, stride=1, padding=1)  # Add padding\n",
    "        self.relu7 = nn.ReLU()\n",
    "\n",
    "        # Layer 8\n",
    "        self.conv8 = nn.Conv1d(384, 384, 1, padding=1)  # Add padding\n",
    "        self.relu8 = nn.ReLU()\n",
    "\n",
    "        # Layer 9\n",
    "        self.conv9 = nn.Conv1d(384, 384, 1, padding=1)  # Add padding\n",
    "        self.relu9 = nn.ReLU()\n",
    "        self.drop9 = nn.Dropout(0.5)\n",
    "\n",
    "        # Layer 10\n",
    "        self.conv10 = nn.Conv1d(384, 20, 3, stride=1, padding=1)  # Add padding\n",
    "        self.relu10 = nn.ReLU()\n",
    "\n",
    "        # Layer 11\n",
    "        self.conv11 = nn.Conv1d(20, 20, 1, padding=1)  # Add padding\n",
    "        self.relu11 = nn.ReLU()\n",
    "\n",
    "        # Layer 12\n",
    "        self.conv12 = nn.Conv1d(20, 20, 1, padding=1)  # Add padding\n",
    "        self.relu12 = nn.ReLU()\n",
    "        self.adapool = nn.AdaptiveAvgPool1d((1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.drop3(self.pool3(self.relu3(self.conv3(x))))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.relu5(self.conv5(x))\n",
    "        x = self.drop6(self.pool6(self.relu6(self.conv6(x))))\n",
    "        x = self.relu7(self.conv7(x))\n",
    "        x = self.relu8(self.conv8(x))\n",
    "        x = self.drop9(self.relu9(self.conv9(x)))\n",
    "        x = self.relu10(self.conv10(x))\n",
    "        x = self.relu11(self.conv11(x))\n",
    "        x = self.adapool(self.relu12(self.conv12(x)))\n",
    "        x = torch.flatten(x, 1)  # Flatten for potential further layers or a classifier\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b68638-b236-4653-9076-60999f85cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: One-hot encoding function\n",
    "def one_hot_encoder(sequence):\n",
    "    mapping = {'A': [1, 0, 0, 0], 'C': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'T': [0, 0, 0, 1]}\n",
    "    return np.array([mapping.get(nucleotide, [0, 0, 0, 0]) for nucleotide in sequence])\n",
    "\n",
    "# Function to read and encode data from a file\n",
    "def read_data_and_encode(file_path):\n",
    "    labels = []\n",
    "    encoded_data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            label, sequence = line.strip().split(' ', 1)\n",
    "            labels.append(int(label))\n",
    "            encoded_data.append(one_hot_encoder(sequence))\n",
    "    return np.array(labels), np.array(encoded_data)\n",
    "\n",
    "# Custom Dataset class\n",
    "class DNADataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.data = torch.stack([torch.tensor(s, dtype=torch.float32) for s in sequences])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        # Reshape data to have 4 channels\n",
    "        data = data.transpose(1, 0)\n",
    "        return data, self.labels[idx]\n",
    "\n",
    "# Main program\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 4: Read and encode data\n",
    "    labels, encoded_data = read_data_and_encode('test_data.txt')\n",
    "    \n",
    "    # Step 8: Instantiate the dataset\n",
    "    test_dataset = DNADataset(encoded_data, labels)\n",
    "    \n",
    "    # Step 9: Create a DataLoader\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "    # Read and encode training data\n",
    "    train_labels, train_encoded_data = read_data_and_encode('train_data.txt')\n",
    "\n",
    "    # Instantiate the training dataset\n",
    "    train_dataset = DNADataset(train_encoded_data, train_labels)\n",
    "\n",
    "    # Create a DataLoader for the training data\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)  \n",
    "    \n",
    "\n",
    "# Instantiating the model and assigning an optimizer to the model and creating a loss function\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer=optim.Adam(params=model.parameters(),lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00526b37-31e3-41f4-90e7-de47a301fde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,device,train_dataloader,optimizer,epochs):\n",
    "    print(\"inside train\")\n",
    "    model.train()\n",
    "    for batch_ids, (img, classes) in enumerate(train_dataloader):\n",
    "        classes=classes.type(torch.LongTensor)\n",
    "        img,classes=img.to(device),classes.to(device)\n",
    "        torch.autograd.set_detect_anomaly(True)     \n",
    "        optimizer.zero_grad()\n",
    "        output=model(img)\n",
    "        loss = loss_fn(output,classes)                \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if(batch_ids +1) % 2 == 0:\n",
    "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "            epochs, batch_ids* len(img), len(train_dataloader.dataset),\n",
    "            100.*batch_ids / len(train_dataloader),loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a42735a-4af5-4fc3-8ab9-9d0246275c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for img, classes in test_dataloader:\n",
    "            img, classes = img.to(device), classes.to(device)\n",
    "            output = model(img)\n",
    "            test_loss += F.cross_entropy(output, classes, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True) \n",
    "            correct += pred.eq(classes.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset)))\n",
    "    print('=' * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5383e398-614f-460d-8774-dac112c94e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inside train\n",
      "\n",
      "Test set: Average loss: 0.5946, Accuracy: 230981/346824 (67%)\n",
      "\n",
      "==============================\n",
      "inside train\n",
      "\n",
      "Test set: Average loss: 0.5795, Accuracy: 235608/346824 (68%)\n",
      "\n",
      "==============================\n",
      "inside train\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    seed=42\n",
    "    EPOCHS=3\n",
    "    \n",
    "    for epoch in range(1,EPOCHS+1):\n",
    "        train(model,device,train_dataloader,optimizer,epoch)\n",
    "        test(model,device,test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07930dea-8c01-4b50-9b3f-cbf501a73c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env] *",
   "language": "python",
   "name": "conda-env-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
